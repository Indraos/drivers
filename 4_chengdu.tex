\chapter{The Value of Information}\label{chap:chengdu}
After evidence from a survey, we move on with evidence derived from platform data. 

we give evidence here on the value of information to drivers. This is aligned with claims by companies that offer information services to drivers \tocite.

\section{Description of Data}
The DiDi KDD Challenge 2020, sponsored by DiDi Yuching, consisted of two challenge. An order dispatch challenge, and a driver repositioning challenge. We will work with data and submissions of the latter.

\subsection{Annotated Order Data}
We are given for November 2016 about %TODO: Find out concrete number
10 Million orders. An order consists of an origin-destination pair, a timestamp for beginning and end, an identifier for the driver that took the order, and a number of \emph{reward units}. As the data is not publicly available anymore, we are unable to provide replication data for the following analysis.

A main challenge in our analysis is to recover the number of reward units corresponding to a RMB. Given our estimation below, we can transfer repositioning scores to monetary values, which gives us an estimate of 

\subsection{Reinforcement Learning Challenge Leaderboard}
The KDD 2020 reinforcement learning challenge solves an optimal repositioning problem together with an optimal order dispatch problem. The problem is a partial observed semi-Markov Decision Process. In Reinforcement Learning language, which we review in \autoref{app:math}.

The system's observable state for the order dispatch algorithm consists of a list of orders to be dispatched at each point in time. This means a list of origin-destination pairs for orders, and for each driver timestamps of \emph{estimated} order pickup and arrival times, as well as the distance for the driver. The reinforcement learning algorithm's actions are matchings of drivers to orders, the challenge does not consider carpooling. Orders need to be dispatched within two seconds, otherwise they are lost. (We specify the complete environment in \autoref{app:kdd}. As part of the state transitions, there is estimated data for cancellation probabilities when sending a driver to pick up orders from different locations.

The objective in the order dipatch algorithm is to maximize average driver income, which, as the system does not model drivers logging off strategically, is proportional to the total driver revenue in terms of orders completed.

The system's observable state for the driver repositioning challenge, which we are most interested in, consists of a set of drivers to be repositioned. There is a targeted group of drivers, which, after their first five minutes of being idle move according to historical transition probabilities between regions, can be repositioned. In addition to a timestamp, the only information given on drivers is a coarse position on a hexagonal grid of the urban area of Chengdu. The actions are, for each driver to be repositioned, a destination location. Agents are then repositioned at 3m/s in the spherical/great arc distance.

The objective in the driver repositioning problem is to maximize mean driver income rate for drivers. Denote $J_k^n (\pi)$ the online time for driver $k$ at day $n$ in hours.\footnote{While the existing documentation do not specify the unit of this measure, our calculations below show that assuming online time is measured in hours leads to correct results.} Denote driver $k$'d income on day $n$ under policy $\pi$. For the set of all targeted drivers $K$ and days $N$, the goal of the challenge was to maximize
\[
	\frac{1}{\lvert K\rvert} \sum_{k \in K} \frac{\sum_{n \in N} J_k^n (\pi)}{\sum_{n \in N} L_k^n (\pi)}.
\]

We claim that the optimal repositioning score for drivers can give insight into the value of information for these drivers, as long as there are not too many of them.

If drivers have sufficient information about the demand throughout the city, they can reposition themselves to another location in expectation of getting higher earnings. As the maximization takes into account all online time, in particular the time moving to another area of the city, the earnings increase from a performant repositioning algorithm in this challenge can also be seen as a proxy for earnings of informed drivers.

In this argument, the small number of repositioned drivers is important. A challenge for drivers, but not for the platform, is that a knowledge on high demand in some area might lead to congestive effects---too many drivers enter high-demand areas.

But even with more drivers, private messages, which we discuss in \autoref{chap:infodesign} help. Private messages is demand information (or, as we show, equivalently recommendations of to which area to move) only given to particular drivers, solving their 

\subsection{Earnings Table}
We are using a publicly available table for earnings from 2021. In 2017, DiDi introduced two tiers of service for their drivers, Express and Premium. After personal conversations with experts, we use the table for express, reproduced in \autoref{tab:earnings}. 

The earnings table contains, dependent of the time of the day, a base price $b_t$, earnings per kilometer $l_t$ and minute driven $d_t$. At the time of the data, November 2016, additionally, a surge multiplier $s_t$. The total earnings $e_t$ for a ride of $L$ kilometers and duration $D$ are given by 
\begin{equation}
	e_t = s_t(b_t + l_t L + d_t D).\label{eq:earnings}
\end{equation}
\begin{table}
\begin{tabular}{cccc}
%TODO: Fill out table.
\end{tabular}
\caption{Earnings for express drivers}
\end{table}
\section{Estimation}
We assume in the following that reward units are a constant multiple of RMB. The underlying assumption is the following proposition.

\begin{prop}
	Assume that reward units $r$ are a function $f$ of the earnings from an order in RMB. Assume that any policy generated for reward units is also optimal for policies in RMB. Then, the function $f$ is a constant multiplication, $f(x) = cx$ for some $c > 0$.
\end{prop}

\subsection{Regression}\label{subsec:chengduregress}
We assume that a negligible fraction of rides uses a surge price and therefore estimate an equation simplifying \eqref{eq:earnings},
\[
	e_t = b_t + l_t L + d_t D.
\]
Our estimate of time is the times given in the order data. We use the Google Maps API to estimate distance in driving. The estimation results are presented in \autoref{tab:rewardunits}. We find highly significant and consistent estimates on the coefficients for distance and duration.


\begin{table}
\centering
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &  reward\_units   & \textbf{  R-squared:         } &     0.887   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.887   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     1380.   \\
\textbf{Date:}             & Sun, 18 Jul 2021 & \textbf{  Prob (F-statistic):} &     0.00    \\
\textbf{Time:}             &     14:45:52     & \textbf{  Log-Likelihood:    } &   -2998.9   \\
\textbf{No. Observations:} &        2000      & \textbf{  AIC:               } &     6004.   \\
\textbf{Df Residuals:}     &        1997      & \textbf{  BIC:               } &     6021.   \\
\textbf{Df Model:}         &           2      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                   & \textbf{coef} & \textbf{std err} & \textbf{z} & \textbf{P$> |$z$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept} &      -0.1664  &        0.077     &    -2.155  &         0.031        &       -0.318    &       -0.015     \\
\textbf{distance}  &       0.3191  &        0.021     &    15.449  &         0.000        &        0.279    &        0.360     \\
\textbf{duration}  &       0.0635  &        0.007     &     9.645  &         0.000        &        0.051    &        0.076     \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Checks}

\begin{table}
\centering
\begin{tabular}{lcccccc}
                                     & \textbf{coef} & \textbf{std err} & \textbf{z} & \textbf{P$> |$z$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                   &      -0.2569  &        0.262     &    -0.981  &         0.326        &       -0.770    &        0.256     \\
\textbf{rush\_hour[T.True]}          &       0.0095  &        0.320     &     0.030  &         0.976        &       -0.618    &        0.637     \\
\textbf{distance}                    &       0.3318  &        0.047     &     7.113  &         0.000        &        0.240    &        0.423     \\
\textbf{rush\_hour[T.True]:distance} &      -0.0292  &        0.048     &    -0.602  &         0.547        &       -0.124    &        0.066     \\
\textbf{duration}                    &       0.0653  &        0.011     &     6.170  &         0.000        &        0.045    &        0.086     \\
\textbf{rush\_hour[T.True]:duration} &       0.0028  &        0.014     &     0.200  &         0.841        &       -0.024    &        0.030     \\
\bottomrule
\end{tabular}
\caption{OLS Regression Results}\label{tab:rewardunits}
\end{table}

\subsection{Model Fit}
We observe that much of the variance can be explained by our model ($R^2 = 0.887$)
\section{Discussion}
