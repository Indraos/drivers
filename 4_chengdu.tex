\chapter{The Value of Information}\label{chap:chengdu}
After our investigation into potential multi-homing frictions in a market, we give evidence here on the value of information to drivers. This is aligned with claims by companies that offer information services to drivers \tocite.

\section{Description of Data}
The DiDi KDD Challenge 2020, sponsored by DiDi Yuching, consisted of two challenge. An order dispatch challenge, and a driver repositioning challenge. We will work with data and submissions of the latter.

\subsection{Annotated Order Data}
We are given for November 2016 about %TODO: Find out concrete number
10 Million orders. An order consists of an origin-destination pair, a timestamp for beginning and end, an identifier for the driver that took the order, and a number of \emph{reward units}. As the data is not publicly available anymore, we are unable to provide replication data for the following analysis.

\subsection{Reinforcement Learning Challenge Leaderboard}
The KDD 2020 reinforcement learning challenge solves an optimal repositioning problem together with an optimal order dispatch problem. The problem is a partial observed semi-Markov Decision Process. In Reinforcement Learning language, which we review in \autoref{app:math}. 

The problem's state is given by

%Each vehicle can serve only one trip at a time, i.e. carpooling is not considered. 
%For any of those vehicles, if the idle time exceeds a threshold of L=5 minutes, the vehicle becomes eligible for repositioning.
%The drivers other than those selected perform idle movement according to a set of generic transition probabilities. The vehicle speed is set at three meters per second for repositioning along spherical distance (a.k.a. great-circle distance). 

%Evaluation
%Each trip order in the released data and the evaluation environment is assigned a reward unit number that represents the potential income for the driver. Evaluation is run over multiple days in simulation, with the same city as the one in the released data set but different time interval. Submissions are evaluated on two metrics corresponding to the two tasks.
%
%Mean total driver income (for order dispatching).
%For each day of simulation run, let  = 1, 路 路 路 ,  be the indices for all the orders submitted to the system. Let ()(, ) be the actual income for order  under order dispatching policy  and vehicle repositioning policy . If order  is completed in the simulation, () is equal to the assigned reward units. Otherwise, () = 0. The evaluation is run over days  = 1, 路 路 路 , . The total income for day  and submission  = (, ) is
%
%The mean total driver income for the submission  is defined as
%
%It is expected that the order dispatching algorithm has the major impact on this metric, as the repositioning algorithm controls only a small group of vehicles, which are unlikely to influence the global context.
% 
%Mean individual income rate (for vehicle repositioning).
%The group of drivers whose repositioning strategy is to be run by  are indexed by  = 1, 路 路 路 , . The total income for driver  on day  is
%
%and the corresponding online time for the driver is
%
%Income rate is defined as the income per unit online time. The mean individual income rate is
%
%The repositioning algorithm is naturally the main influencer on this metric, as the order dispatching algorithm is agnostic to the identity of this group of drivers.
The order dispatch problem is a reinforcement Learning Problem $(S, \mathcal A, T, R, f)$. The state is given locations of idle drivers as a list of orders. The actions are a (potentially partial) matching of available drivers to orders. All orders must be matched within a 2 second window, and otherwise fall through. 


We also use the leaderboard's reinforcement challenge leaderboard. It is taken from \tocite, table 5, for the case of where 2\% of drivers are repositioned.
\subsection{Earnings Table}
We are using a publicly available table for earnings from 2021. In 2017, DiDi introduced two tiers of service for their drivers, Express 


%(S, A, T, R, 纬)
%https://github.com/mktal/kddcup-starting-kit/tree/master/model
%  def dispatch(self, dispatch_observ):
%    """ Compute the assignment between drivers and passengers at each time step
%    :param dispatch_observ: a list of dict, the key in the dict includes:
%        order_id, int
%        driver_id, int
%        order_driver_distance, estimated distance between the driver and the order, float
%        order_start_location, a list as [lng, lat], float
%        order_finish_location, a list as [lng, lat], float
%        driver_location, a list as [lng, lat], float
%        timestamp, current simulation time, int
%        order_finish_timestamp, estimated order finish time, int
%        day_of_week, Monday=0, Sunday=6, int
%        reward_units, reward received after the order is completed, float
%        pick_up_eta, estimated time (in seconds) it takes the driver to pick up the order, float
%
%    :return: a list of dict, the key in the dict includes:
%        order_id and driver_id, the pair indicating the assignment
%    """
%    pass
%
%  def reposition(self, repo_observ):
%    """ Compute the reposition action for the given drivers
%    :param repo_observ: a dict, the key in the dict includes:
%        timestamp: int
%        driver_info: a list of dict, the key in the dict includes:
%            driver_id: id of the idle driver in the treatment group, int
%            grid_id: id of the grid the driver is located at, str
%        day_of_week: int
%
%    :return: a list of dict, the key in the dict includes:
%        driver_id: id of the idle driver in the treatment group, int
%        destination: id of the grid the driver is repositioned to, str
%    """
%    pass
%TODO: Visualize table.

%https://www.biendata.xyz/competition/kdd_didi/evaluation/
%https://pubs.aeaweb.org/doi/pdfplus/10.1257/jel.20181489
\section{Estimation}
\subsection{Model}
We assume that there is a constant $c$ such that 
\[
\operatorname{reward units} = c \operatorname{RMB}.
\]
The reason for this lies in the following proposition.

\subsection{Regression}\label{subsec:chengduregress}






\begin{table}
\centering
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &  reward\_units   & \textbf{  R-squared:         } &     0.887   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.887   \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     1380.   \\
\textbf{Date:}             & Sun, 18 Jul 2021 & \textbf{  Prob (F-statistic):} &     0.00    \\
\textbf{Time:}             &     14:45:52     & \textbf{  Log-Likelihood:    } &   -2998.9   \\
\textbf{No. Observations:} &        2000      & \textbf{  AIC:               } &     6004.   \\
\textbf{Df Residuals:}     &        1997      & \textbf{  BIC:               } &     6021.   \\
\textbf{Df Model:}         &           2      & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                   & \textbf{coef} & \textbf{std err} & \textbf{z} & \textbf{P$> |$z$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept} &      -0.1664  &        0.077     &    -2.155  &         0.031        &       -0.318    &       -0.015     \\
\textbf{distance}  &       0.3191  &        0.021     &    15.449  &         0.000        &        0.279    &        0.360     \\
\textbf{duration}  &       0.0635  &        0.007     &     9.645  &         0.000        &        0.051    &        0.076     \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Checks}

\begin{table}
\centering
\begin{tabular}{lcccccc}
                                     & \textbf{coef} & \textbf{std err} & \textbf{z} & \textbf{P$> |$z$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                   &      -0.2569  &        0.262     &    -0.981  &         0.326        &       -0.770    &        0.256     \\
\textbf{rush\_hour[T.True]}          &       0.0095  &        0.320     &     0.030  &         0.976        &       -0.618    &        0.637     \\
\textbf{distance}                    &       0.3318  &        0.047     &     7.113  &         0.000        &        0.240    &        0.423     \\
\textbf{rush\_hour[T.True]:distance} &      -0.0292  &        0.048     &    -0.602  &         0.547        &       -0.124    &        0.066     \\
\textbf{duration}                    &       0.0653  &        0.011     &     6.170  &         0.000        &        0.045    &        0.086     \\
\textbf{rush\_hour[T.True]:duration} &       0.0028  &        0.014     &     0.200  &         0.841        &       -0.024    &        0.030     \\
\bottomrule
\end{tabular}
%\caption{OLS Regression Results}
\end{table}

\section{Discussion}
