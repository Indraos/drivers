\chapter{The Value of Information}\label{chap:chengdu}
While the last chapter investigated stated aspects of relevance for labor supply, this section shows the \emph{potential} effects of platform design beyond matching and pricing.

In this chapter, we give evidence that demand information can significantly increase earnings for platform drivers. The earnings gains are aligned with claims by companies that offer information services to drivers such as Gridwise or the Mileage tracker that additional information can increase earnings. We first describe data that we use in \autoref{sec:chengdudata}, we then estimate driver earnings in optimized repositioning in \autoref{sec:chengduestimation} and close with comparing this with driver income under non-optimized behavior and discuss our results in \autoref{sec:chengdudiscussion}.
\section{Description of Data}\label{sec:chengdudata}
The DiDi KDD Challenge 2020, sponsored by DiDi Yuching, consisted of two challenge. An order dispatch challenge, and a driver repositioning challenge. We will work with data and submissions of the latter.

\subsection{Annotated Order Data}
We use a dataset of 14,131,874 orders dispatched in Chengdu, China, in November 2016. An order consists of an origin-destination pair, a timestamp for beginning and end, an identifier for the driver that took the order, and a number of \emph{reward units}. As the data is not publicly available anymore, we are unable to provide replication data for the following analysis.

A main challenge in our analysis is to recover the number of reward units corresponding to a RMB. Given our estimation below, we can transfer repositioning scores to monetary values, which gives us an estimate of 

\subsection{Evaluation in Paper}
The KDD 2020 reinforcement learning challenge solves an optimal repositioning problem together with an optimal order dispatch problem. The problem is a partial observed semi-Markov Decision Process. In Reinforcement Learning language, which we review in \autoref{app:math}.

The system's observable state for the order dispatch algorithm consists of a list of orders to be dispatched at each point in time. This means a list of origin-destination pairs for orders, and for each driver timestamps of \emph{estimated} order pickup and arrival times, as well as the distance for the driver. The reinforcement learning algorithm's actions are matchings of drivers to orders, the challenge does not consider carpooling. Orders need to be dispatched within two seconds, otherwise they are lost. (We specify the complete environment in \autoref{app:kdd}. As part of the state transitions, there is estimated data for cancellation probabilities when sending a driver to pick up orders from different locations.

The objective in the order dipatch algorithm is to maximize average driver income, which, as the system does not model drivers logging off strategically, is proportional to the total driver revenue in terms of orders completed.

The system's observable state for the driver repositioning challenge, which we are most interested in, consists of a set of drivers to be repositioned. There is a targeted group of drivers, which, after their first five minutes of being idle move according to historical transition probabilities between regions, can be repositioned. In addition to a timestamp, the only information given on drivers is a coarse position on a hexagonal grid of the urban area of Chengdu. The actions are, for each driver to be repositioned, a destination location. Agents are then repositioned at 3m/s in the spherical/great arc distance.

The objective in the driver repositioning problem is to maximize mean driver income rate for drivers. Denote $J_k^n (\pi)$ the online time for driver $k$ at day $n$ in hours.\footnote{While the existing documentation do not specify the unit of this measure, our calculations below show that assuming online time is measured in hours leads to correct results.} Denote driver $k$'d income on day $n$ under policy $\pi$. For the set of all targeted drivers $K$ and days $N$, the goal of the challenge was to maximize
\[
	\frac{1}{\lvert K\rvert} \sum_{k \in K} \frac{\sum_{n \in N} J_k^n (\pi)}{\sum_{n \in N} L_k^n (\pi)}.
\]

We claim that the optimal repositioning score for drivers can give insight into the value of information for these drivers, as long as there are not too many of them.

If drivers have sufficient information about the demand throughout the city, they can reposition themselves to another location in expectation of getting higher earnings. As the maximization takes into account all online time, in particular the time moving to another area of the city, the earnings increase from a performant repositioning algorithm in this challenge can also be seen as a proxy for earnings of informed drivers.

In this argument, the small number of repositioned drivers is important. A challenge for drivers, but not for the platform, is that a knowledge on high demand in some area might lead to congestive effects---too many drivers enter high-demand areas.

But even with more drivers, private messages, which we discuss in \autoref{chap:infodesign} help. Private messages is demand information (or, as we show, equivalently recommendations of to which area to move) only given to particular drivers, solving their 

\subsection{Earnings Table}
We are using a publicly available table for earnings from 2021. In 2017, DiDi introduced two tiers of service for their drivers, Express and Premium. After personal conversations with experts, we use the table for express, reproduced in \autoref{tab:earnings}. 

The earnings table contains, dependent of the time of the day, a base price $b_t$, earnings per kilometer $l_t$ and minute driven $d_t$. At the time of the data, November 2016, additionally, a surge multiplier $s_t$. The total earnings $e_t$ for a ride of $L$ kilometers and duration $D$ are given by 
\begin{equation}
	e_t = s_t(b_t + l_t L + d_t D).\label{eq:earnings}
\end{equation}
\begin{table}[]
\centering
\begin{tabular}{lllll}
\toprule
 & Base Price & Price/km & Price/min &  \\
\midrule
$22:00-07:00$  & 12.40      & 2.70     & 0.48      &  \\
$07:00-10:00$  & 12.40      & 2.55     & 0.48      &  \\
$10:00-16:00$ & 11.40      & 1.95     & 0.42      &  \\
$16:00-19:00$ & 12.40      & 2.33     & 0.48      &  \\
$19:00-22:00$ & 11.40      & 1.95     & 0.42      & \\
\bottomrule
\end{tabular}
\caption{Earnings for express drivers}
\end{table}
\section{Estimation}\label{sec:chengduestimation}
We assume in the following that reward units are a constant multiple of RMB. The underlying assumption is the following proposition.

\begin{prop}
	Assume that reward units $r$ are a function $f$ of the earnings from an order in RMB. Assume that any policy generated for reward units is also optimal for policies in RMB. Then, the function $f$ is a constant multiplication, $f(x) = cx$ for some $c > 0$.
\end{prop}

\subsection{Regression}\label{subsec:chengduregress}
We assume that a negligible fraction of rides uses a surge price and therefore estimate an equation simplifying \eqref{eq:earnings},
\[
	e_t = b_t + l_t L + d_t D.
\]
Our estimate of time is the times given in the order data. We use the Google Maps API to estimate distance in driving. The estimation results are presented in \autoref{tab:rewardunits}. We find highly significant and consistent estimates on the coefficients for distance and duration.

\begin{table}[!htbp] \centering
\begin{tabular}{@{\extracolsep{5pt}}lcc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
& \multicolumn{2}{c}{Dependent variable: Reward Units} \
\cr \cline{2-3}
\\[-1.8ex] & (1) & (2) \\
\hline \\[-1.8ex]
 Intercept & 0.751$^{***}$ & 0.703$^{***}$ \\
  & (0.042) & (0.041) \\
 distance & 0.290$^{***}$ & 0.317$^{***}$ \\
  & (0.014) & (0.076) \\
 duration & 0.065$^{***}$ & 0.116$^{***}$ \\
  & (0.005) & (0.041) \\
 hour[T.22-7]:distance & & -0.043$^{}$ \\
  & & (0.077) \\
 hour[T.22-7]:duration & & -0.050$^{}$ \\
  & & (0.041) \\
 hour[T.7-10]:distance & & -0.067$^{}$ \\
  & & (0.089) \\
 hour[T.7-10]:duration & & -0.037$^{}$ \\
  & & (0.043) \\
 hour[T.offhour]:distance & & 0.003$^{}$ \\
  & & (0.078) \\
 hour[T.offhour]:duration & & -0.057$^{}$ \\
  & & (0.041) \\
\hline \\[-1.8ex]
 Observations & 3,997 & 3,997 \\
 $R^2$ & 0.883 & 0.894 \\
 Adjusted $R^2$ & 0.883 & 0.894 \\
\hline
\hline \\[-1.8ex]
\textit{Note:} & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\
\end{tabular}
\end{table}
We observe that much of the variance can be explained by our model ($R^2 = 0.887$). As another sanity check, we infer the distribution of surge prices throughout the day. 
\section{Discussion}\label{sec:chengdudiscussion}
We see that optimal driver repositioning has a significant effect on driver earnings if only a few drivers are treated to this additional information. 

In a model with more drivers being treated, informed drivers might \emph{crowd out} each other, and earnings differential might be smaller.

This does not mean that information design is not possible to improve platforms, but that, for each trip, only a subset of drivers may be informed of it. This robust property is one of our findings in the next chapter.